{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b34ef03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn, optim\n",
    "from PIL import Image\n",
    "from matplotlib.pyplot import imshow\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e798657",
   "metadata": {},
   "source": [
    "### Connect to gpu if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1262985a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7099b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell to generate csv file that contains all image paths and labels\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "dataset_df = pd.DataFrame(columns=[\"img_name\",\"label\"])\n",
    "\n",
    "img_data_dir = './Task'\n",
    "classes = os.listdir(img_data_dir)\n",
    "\n",
    "i = 0;\n",
    "for label in classes:  ## looping over each class\n",
    "\n",
    "    class_path = img_data_dir + \"/\" + label  ## create the path of each image class\n",
    "\n",
    "    files = os.listdir(class_path)  ## list of all files in each image class folder\n",
    "\n",
    "    for file in files:  ## Enumerate over each image list\n",
    "        dataset_df.loc[i] = [ os.path.join(class_path, file), ord(label) - ord('a')]\n",
    "        i += 1\n",
    "        \n",
    "dataset_df.to_csv ('dataset_csv.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f735369",
   "metadata": {},
   "source": [
    "### Set the root folder and dimensions of each image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "715ae4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"./Task\"\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5,), (0.5,)),\n",
    "                              ])\n",
    "image_length=128\n",
    "image_width=128\n",
    "number_of_classes=26"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdfdd1e",
   "metadata": {},
   "source": [
    "### Craete a class that inherits from  torch.utils.data.Dataset to load a custom data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b7f72e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, root, transform, annotation_file):\n",
    "        \"\"\"\n",
    "        :param root: the directory where the dataset is.\n",
    "        :param transform: transformation that is needed to be applied to data before it is loaded by DataLoader.\n",
    "        :param annotation_file: csv file that holds the path and the label for each image.\n",
    "        \"\"\"\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.annotations = pd.read_csv(annotation_file)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        :return: the length of dataset, the number of data in dataset\n",
    "        \"\"\"\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        :param idx: index of data and corresponding label that we want to retrieve from dataset\n",
    "        :return: x --> the tesnor image of that index, y --> the label of that index\n",
    "        \"\"\"\n",
    "        path = self.annotations.iloc[idx, 0]\n",
    "        img = Image.open(path).convert('L')  ## each the images from the folder. We are passing the file path + name in \"imread\"\n",
    "        img = transform(img)\n",
    "        x = img\n",
    "        y = torch.tensor(self.annotations.iloc[idx, 1], dtype=torch.long)\n",
    "\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58746027",
   "metadata": {},
   "source": [
    "### Declare the root folder and declare a new instance of the new class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2452e8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_file = './dataset_csv.csv'\n",
    "dataset = MyDataset(root, transform, dataset_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86673f2e",
   "metadata": {},
   "source": [
    "### Split the data into train and validation sets,  create a data loader for each set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "571d5d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label =  v\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_loader = torch.utils.data.DataLoader(dataset)\n",
    "\n",
    "train_size = int(0.8 * len(data_loader))\n",
    "val_size = len(data_loader) - train_size\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "valloader = torch.utils.data.DataLoader(val_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "images, labels = next(iter(trainloader))\n",
    "\n",
    "## show a sample image\n",
    "print(\"Label = \", chr(labels[0] + ord('a')) )\n",
    "imshow(transforms.ToPILImage()(images[0]), cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36be796c",
   "metadata": {},
   "source": [
    "### Build the neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3d4a1a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=16384, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=64, out_features=26, bias=True)\n",
      "  (5): LogSoftmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "input_size = image_width*image_width\n",
    "hidden_sizes = [128, 64]\n",
    "output_size = number_of_classes\n",
    "\n",
    "model = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[1], output_size),\n",
    "                      nn.LogSoftmax(dim=1))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a3cbea",
   "metadata": {},
   "source": [
    "### Test if everything is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "609e5fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "images, labels = next(iter(trainloader))\n",
    "images = images.view(images.shape[0], -1)\n",
    "\n",
    "logps = model(images) #log probabilities\n",
    "loss = criterion(logps, labels) #calculate the NLL loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5a1340",
   "metadata": {},
   "source": [
    "### The train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a3eab4f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Training loss: 1.670554875649003\n",
      "Epoch 1 - Training loss: 1.003409009498598\n",
      "Epoch 2 - Training loss: 0.8159402048316394\n",
      "Epoch 3 - Training loss: 0.7249936859160923\n",
      "Epoch 4 - Training loss: 0.6669126106279326\n",
      "Epoch 5 - Training loss: 0.6180345008309585\n",
      "Epoch 6 - Training loss: 0.5888698937309393\n",
      "Epoch 7 - Training loss: 0.5674817070834691\n",
      "Epoch 8 - Training loss: 0.5552426192080911\n",
      "Epoch 9 - Training loss: 0.5313527806158033\n",
      "Epoch 10 - Training loss: 0.5176496311317601\n",
      "Epoch 11 - Training loss: 0.5080646556751531\n",
      "Epoch 12 - Training loss: 0.49129337079549\n",
      "Epoch 13 - Training loss: 0.4848606320792865\n",
      "Epoch 14 - Training loss: 0.4751515891500822\n",
      "\n",
      "Training Time (in minutes) = 77.5621245265007\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.003, momentum=0.9)\n",
    "time0 = time()\n",
    "epochs = 15\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in trainloader:\n",
    "        # Flatten images into a 128*128 long vector\n",
    "        images = images.view(images.shape[0], -1)\n",
    "    \n",
    "        # Training pass\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(images)\n",
    "        loss = criterion(output, labels)\n",
    "        \n",
    "        #This is where the model learns by backpropagating\n",
    "        loss.backward()\n",
    "        \n",
    "        #And optimizes its weights here\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        print(\"Epoch {} - Training loss: {}\".format(e, running_loss/len(trainloader)))\n",
    "print(\"\\nTraining Time (in minutes) =\",(time()-time0)/60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21b7dd4",
   "metadata": {},
   "source": [
    "### Perfrom a prediction on a sample image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "56f75226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Digit = 17 label = tensor(17)\n"
     ]
    }
   ],
   "source": [
    "images, labels = next(iter(valloader))\n",
    "\n",
    "img = images[0].view(1, image_length*image_width)\n",
    "with torch.no_grad():\n",
    "    logps = model(img)\n",
    "\n",
    "ps = torch.exp(logps)\n",
    "probab = list(ps.numpy()[0])\n",
    "print(\"Predicted Digit =\", probab.index(max(probab)), \"label =\", labels[0].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea34bf0",
   "metadata": {},
   "source": [
    "### save the model for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b020fe6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, './my_model.pt') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d8b2f2",
   "metadata": {},
   "source": [
    "### Validate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5a6c37ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels = []\n",
    "true_labels = []\n",
    "\n",
    "for images,labels in valloader:\n",
    "    for i in range(len(labels)):\n",
    "        img = images[i].view(1, image_length*image_width)\n",
    "        with torch.no_grad():\n",
    "            logps = model(img)\n",
    "\n",
    "        ps = torch.exp(logps)\n",
    "        pred_labels.append(ps.argmax().item())\n",
    "        true_labels.append(labels[i].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164e684c",
   "metadata": {},
   "source": [
    "### Report Model Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "92024464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.86      0.86      1026\n",
      "           1       0.90      0.92      0.91       988\n",
      "           2       0.81      0.92      0.86       584\n",
      "           3       0.91      0.93      0.92       970\n",
      "           4       0.94      0.90      0.92      1015\n",
      "           5       0.83      0.89      0.86       510\n",
      "           6       0.77      0.60      0.68       774\n",
      "           7       0.89      0.87      0.88       981\n",
      "           8       0.74      0.61      0.67       521\n",
      "           9       0.94      0.62      0.75       400\n",
      "          10       0.77      0.87      0.82       496\n",
      "          11       0.84      0.96      0.90      1043\n",
      "          12       0.97      0.90      0.93       534\n",
      "          13       0.88      0.94      0.91       978\n",
      "          14       0.86      0.90      0.88       553\n",
      "          15       0.82      0.90      0.86       465\n",
      "          16       0.57      0.81      0.67       589\n",
      "          17       0.92      0.95      0.93      1005\n",
      "          18       0.95      0.86      0.90       562\n",
      "          19       0.87      0.91      0.89      1036\n",
      "          20       0.86      0.80      0.83       550\n",
      "          21       0.86      0.81      0.83       582\n",
      "          22       0.92      0.93      0.92       514\n",
      "          23       0.88      0.83      0.85       563\n",
      "          24       0.96      0.57      0.71       490\n",
      "          25       0.88      0.92      0.90       531\n",
      "\n",
      "    accuracy                           0.86     18260\n",
      "   macro avg       0.86      0.84      0.85     18260\n",
      "weighted avg       0.87      0.86      0.86     18260\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(true_labels, pred_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4141a3b9",
   "metadata": {},
   "source": [
    "### References that helped me through the task\n",
    "- https://medium.com/analytics-vidhya/implementing-cnn-in-pytorch-with-custom-dataset-and-transfer-learning-1864daac14cc\n",
    "- https://towardsdatascience.com/handwritten-digit-mnist-pytorch-977b5338e627"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
